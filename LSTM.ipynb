{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MC_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(MC_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.word_embeddings(text)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(text), 1, -1), self.hidden)\n",
    "        out = self.fc(lstm_out.view(len(text), -1))\n",
    "        return F.log_softmax(out, dim=1)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('texts/necronomicon.txt','r').read()\n",
    "data = data.split()\n",
    "\n",
    "vocab = list(set(data))\n",
    "word_to_ix = {k:v for v,k in enumerate(vocab)}\n",
    "ix_to_word = {k:v for k,v in enumerate(vocab)}\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "text = [w for w in data[:-1]]\n",
    "target = [t for t in data[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = open('texts/without_me.txt','r').read()\n",
    "data2 = data2.split()\n",
    "\n",
    "vocab2 = list(set(data2))\n",
    "word_to_ix2 = {k:v for v,k in enumerate(vocab2)}\n",
    "ix_to_word2 = {k:v for k,v in enumerate(vocab2)}\n",
    "text2 = [w for w in data2[:-1]]\n",
    "target2 = [t for t in data2[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ix = prepare_sequence(text,word_to_ix)\n",
    "target_ix = prepare_sequence(target,word_to_ix)\n",
    "\n",
    "text_ix2 = prepare_sequence(text2,word_to_ix2)\n",
    "target_ix2 = prepare_sequence(target2,word_to_ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixds =([text_ix,target_ix],[text_ix2,target_ix2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MC_LSTM(EMBEDDING_DIM,HIDDEN_DIM,len(vocab),len(target))\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " Loss: 9.35\n",
      "===============\n",
      "Epoch 2/100\n",
      " Loss: 9.24\n",
      "===============\n",
      "Epoch 3/100\n",
      " Loss: 9.06\n",
      "===============\n",
      "Epoch 4/100\n",
      " Loss: 8.70\n",
      "===============\n",
      "Epoch 5/100\n",
      " Loss: 8.10\n",
      "===============\n",
      "Epoch 6/100\n",
      " Loss: 7.59\n",
      "===============\n",
      "Epoch 7/100\n",
      " Loss: 7.22\n",
      "===============\n",
      "Epoch 8/100\n",
      " Loss: 6.96\n",
      "===============\n",
      "Epoch 9/100\n",
      " Loss: 6.79\n",
      "===============\n",
      "Epoch 10/100\n",
      " Loss: 6.71\n",
      "===============\n",
      "Epoch 11/100\n",
      " Loss: 6.66\n",
      "===============\n",
      "Epoch 12/100\n",
      " Loss: 6.65\n",
      "===============\n",
      "Epoch 13/100\n",
      " Loss: 6.65\n",
      "===============\n",
      "Epoch 14/100\n",
      " Loss: 6.63\n",
      "===============\n",
      "Epoch 15/100\n",
      " Loss: 6.60\n",
      "===============\n",
      "Epoch 16/100\n",
      " Loss: 6.55\n",
      "===============\n",
      "Epoch 17/100\n",
      " Loss: 6.51\n",
      "===============\n",
      "Epoch 18/100\n",
      " Loss: 6.46\n",
      "===============\n",
      "Epoch 19/100\n",
      " Loss: 6.42\n",
      "===============\n",
      "Epoch 20/100\n",
      " Loss: 6.38\n",
      "===============\n",
      "Epoch 21/100\n",
      " Loss: 6.33\n",
      "===============\n",
      "Epoch 22/100\n",
      " Loss: 6.28\n",
      "===============\n",
      "Epoch 23/100\n",
      " Loss: 6.24\n",
      "===============\n",
      "Epoch 24/100\n",
      " Loss: 6.19\n",
      "===============\n",
      "Epoch 25/100\n",
      " Loss: 6.14\n",
      "===============\n",
      "Epoch 26/100\n",
      " Loss: 6.09\n",
      "===============\n",
      "Epoch 27/100\n",
      " Loss: 6.04\n",
      "===============\n",
      "Epoch 28/100\n",
      " Loss: 5.99\n",
      "===============\n",
      "Epoch 29/100\n",
      " Loss: 5.93\n",
      "===============\n",
      "Epoch 30/100\n",
      " Loss: 5.87\n",
      "===============\n",
      "Epoch 31/100\n",
      " Loss: 5.82\n",
      "===============\n",
      "Epoch 32/100\n",
      " Loss: 5.76\n",
      "===============\n",
      "Epoch 33/100\n",
      " Loss: 5.70\n",
      "===============\n",
      "Epoch 34/100\n",
      " Loss: 5.64\n",
      "===============\n",
      "Epoch 35/100\n",
      " Loss: 5.58\n",
      "===============\n",
      "Epoch 36/100\n",
      " Loss: 5.52\n",
      "===============\n",
      "Epoch 37/100\n",
      " Loss: 5.45\n",
      "===============\n",
      "Epoch 38/100\n",
      " Loss: 5.39\n",
      "===============\n",
      "Epoch 39/100\n",
      " Loss: 5.32\n",
      "===============\n",
      "Epoch 40/100\n",
      " Loss: 5.26\n",
      "===============\n",
      "Epoch 41/100\n",
      " Loss: 5.19\n",
      "===============\n",
      "Epoch 42/100\n",
      " Loss: 5.12\n",
      "===============\n",
      "Epoch 43/100\n",
      " Loss: 5.05\n",
      "===============\n",
      "Epoch 44/100\n",
      " Loss: 4.98\n",
      "===============\n",
      "Epoch 45/100\n",
      " Loss: 4.91\n",
      "===============\n",
      "Epoch 46/100\n",
      " Loss: 4.84\n",
      "===============\n",
      "Epoch 47/100\n",
      " Loss: 4.77\n",
      "===============\n",
      "Epoch 48/100\n",
      " Loss: 4.70\n",
      "===============\n",
      "Epoch 49/100\n",
      " Loss: 4.63\n",
      "===============\n",
      "Epoch 50/100\n",
      " Loss: 4.56\n",
      "===============\n",
      "Epoch 51/100\n",
      " Loss: 4.48\n",
      "===============\n",
      "Epoch 52/100\n",
      " Loss: 4.41\n",
      "===============\n",
      "Epoch 53/100\n",
      " Loss: 4.34\n",
      "===============\n",
      "Epoch 54/100\n",
      " Loss: 4.27\n",
      "===============\n",
      "Epoch 55/100\n",
      " Loss: 4.21\n",
      "===============\n",
      "Epoch 56/100\n",
      " Loss: 4.14\n",
      "===============\n",
      "Epoch 57/100\n",
      " Loss: 4.07\n",
      "===============\n",
      "Epoch 58/100\n",
      " Loss: 4.00\n",
      "===============\n",
      "Epoch 59/100\n",
      " Loss: 3.94\n",
      "===============\n",
      "Epoch 60/100\n",
      " Loss: 3.87\n",
      "===============\n",
      "Epoch 61/100\n",
      " Loss: 3.81\n",
      "===============\n",
      "Epoch 62/100\n",
      " Loss: 3.75\n",
      "===============\n",
      "Epoch 63/100\n",
      " Loss: 3.69\n",
      "===============\n",
      "Epoch 64/100\n",
      " Loss: 3.63\n",
      "===============\n",
      "Epoch 65/100\n",
      " Loss: 3.58\n",
      "===============\n",
      "Epoch 66/100\n",
      " Loss: 3.52\n",
      "===============\n",
      "Epoch 67/100\n",
      " Loss: 3.47\n",
      "===============\n",
      "Epoch 68/100\n",
      " Loss: 3.42\n",
      "===============\n",
      "Epoch 69/100\n",
      " Loss: 3.37\n",
      "===============\n",
      "Epoch 70/100\n",
      " Loss: 3.32\n",
      "===============\n",
      "Epoch 71/100\n",
      " Loss: 3.27\n",
      "===============\n",
      "Epoch 72/100\n",
      " Loss: 3.23\n",
      "===============\n",
      "Epoch 73/100\n",
      " Loss: 3.18\n",
      "===============\n",
      "Epoch 74/100\n",
      " Loss: 3.14\n",
      "===============\n",
      "Epoch 75/100\n",
      " Loss: 3.10\n",
      "===============\n",
      "Epoch 76/100\n",
      " Loss: 3.06\n",
      "===============\n",
      "Epoch 77/100\n",
      " Loss: 3.02\n",
      "===============\n",
      "Epoch 78/100\n",
      " Loss: 2.98\n",
      "===============\n",
      "Epoch 79/100\n",
      " Loss: 2.95\n",
      "===============\n",
      "Epoch 80/100\n",
      " Loss: 2.91\n",
      "===============\n",
      "Epoch 81/100\n",
      " Loss: 2.88\n",
      "===============\n",
      "Epoch 82/100\n",
      " Loss: 2.84\n",
      "===============\n",
      "Epoch 83/100\n",
      " Loss: 2.81\n",
      "===============\n",
      "Epoch 84/100\n",
      " Loss: 2.78\n",
      "===============\n",
      "Epoch 85/100\n",
      " Loss: 2.75\n",
      "===============\n",
      "Epoch 86/100\n",
      " Loss: 2.72\n",
      "===============\n",
      "Epoch 87/100\n",
      " Loss: 2.69\n",
      "===============\n",
      "Epoch 88/100\n",
      " Loss: 2.66\n",
      "===============\n",
      "Epoch 89/100\n",
      " Loss: 2.63\n",
      "===============\n",
      "Epoch 90/100\n",
      " Loss: 2.61\n",
      "===============\n",
      "Epoch 91/100\n",
      " Loss: 2.58\n",
      "===============\n",
      "Epoch 92/100\n",
      " Loss: 2.56\n",
      "===============\n",
      "Epoch 93/100\n",
      " Loss: 2.53\n",
      "===============\n",
      "Epoch 94/100\n",
      " Loss: 2.51\n",
      "===============\n",
      "Epoch 95/100\n",
      " Loss: 2.48\n",
      "===============\n",
      "Epoch 96/100\n",
      " Loss: 2.46\n",
      "===============\n",
      "Epoch 97/100\n",
      " Loss: 2.44\n",
      "===============\n",
      "Epoch 98/100\n",
      " Loss: 2.42\n",
      "===============\n",
      "Epoch 99/100\n",
      " Loss: 2.40\n",
      "===============\n",
      "Epoch 100/100\n",
      " Loss: 2.38\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    #ix = ixds[np.random.randint(0,2)]\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.hidden = model.init_hidden()\n",
    "\n",
    "    out = model(text_ix)    \n",
    "\n",
    "    loss = criterion(out,target_ix)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    print(\"Epoch {}/{}\\n Loss: {:.2f}\".format(epoch+1,epochs,loss.data[0]))\n",
    "    print(\"=\"*15)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(text_ix)\n",
    "_ ,ix = out.max(1)\n",
    "        \n",
    "lyrics = [ix_to_word[w.data[0]] for w in ix]\n",
    "\n",
    "with open('texts/output.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.writelines(' '.join(lyrics))\n",
    "    myfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(prepare_sequence(['black'],word_to_ix))\n",
    "_ ,ix = out.max(1)\n",
    "\n",
    "idxs = []\n",
    "\n",
    "for _ in range(5000):\n",
    "\n",
    "    out = model(ix)\n",
    "    _ ,ix = out.max(1)\n",
    "    \n",
    "    idxs.append(ix)\n",
    "    \n",
    "        \n",
    "lyrics = [ix_to_word[w.data[0]] for w in idxs]\n",
    "\n",
    "with open('texts/output.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.writelines(' '.join(lyrics))\n",
    "    myfile.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
