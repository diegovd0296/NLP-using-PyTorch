{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    DTYPE = torch.cuda.FloatTensor\n",
    "else:\n",
    "    DTYPE = torch.FloatTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('texts/Lovecraft.txt','r').read().lower()\n",
    "alphabet = set(data)\n",
    "\n",
    "ix_to_char = {k:v for k,v in enumerate(alphabet)}\n",
    "char_to_ix = {k:v for v,k in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to prepare sequences\n",
    "\n",
    "def prepare_seq(data, drop_last=False):\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    # Create (len(data)/SEQ_LEN+1) number of vectors of SEQ_LEN lenght\n",
    "    for i in range(0,len(data),SEQ_LEN+1):\n",
    "\n",
    "        chars = [char_to_ix[c] for c in data[i:i+SEQ_LEN+1]]\n",
    "        sequences.append(chars)\n",
    "    \n",
    "    # Drop last batch if incomplete`\n",
    "    if drop_last and len(sequences) % BATCH_SIZE != 0:\n",
    "        \n",
    "        index = len(sequences)//BATCH_SIZE * BATCH_SIZE\n",
    "        del(sequences[index:])\n",
    "    \n",
    "    \n",
    "    # Drop last sequence if incomplete\n",
    "    elif len(sequences[-1]) != SEQ_LEN:\n",
    "        del(sequences[-1])\n",
    "    \n",
    "    sequences = np.array([sequences]).reshape((-1,SEQ_LEN+1))\n",
    "    \n",
    "    # Create inputs and targets\n",
    "    inputs = sequences[:,:-1]\n",
    "    targets = sequences[:,1:]\n",
    "    \n",
    "    \n",
    "    # Convert sequences to variables\n",
    "    sequences =  autograd.Variable(torch.Tensor(sequences).type(DTYPE))\n",
    "    inputs = autograd.Variable(torch.Tensor(inputs).type(DTYPE))\n",
    "    targets = autograd.Variable(torch.Tensor(targets).type(DTYPE))    \n",
    "    \n",
    "    \n",
    "    return inputs,targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim,hidden_dim2 ,output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.hidden_dim2 = hidden_dim2\n",
    "        self.lstm = nn.LSTM(input_size,hidden_dim,NUM_LAYERS)\n",
    "        #self.lstm2 = nn.LSTM(hidden_dim,hidden_dim2)\n",
    "        self.h2O = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.init_hidden(self.hidden_dim)\n",
    "        #self.hidden2 = self.init_hidden(self.hidden_dim2)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self,hidden_dim):\n",
    "       \n",
    "        return (autograd.Variable(torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_dim).type(DTYPE)),\n",
    "                autograd.Variable(torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_dim).type(DTYPE)))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, self.hidden = self.lstm(sequence,self.hidden)\n",
    "        #lstm_out, self.hidden2 = self.lstm2(lstm_out,self.hidden2)\n",
    "        out = self.h2O(lstm_out.view(-1,self.hidden_dim))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 128\n",
    "HIDDEN_DIM2 = 128\n",
    "SEQ_LEN = 64\n",
    "\n",
    "\n",
    "input_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = LSTM(input_size,HIDDEN_DIM,HIDDEN_DIM2,len(alphabet)).type(DTYPE)\n",
    "optimizer = optim.Adam(rnn.parameters(),lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    \n",
    "    inputs,targets = prepare_seq(data,drop_last=True)\n",
    "    \n",
    "    rnn.train(True)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        losses = np.array([])\n",
    "\n",
    "        #permutation = torch.randperm(sequence.size()[0]).type(DTYPE).long()    \n",
    "\n",
    "        for i in range(0,inputs.size()[0],BATCH_SIZE):\n",
    "\n",
    "            rnn.zero_grad()\n",
    "            rnn.hidden = rnn.init_hidden(rnn.hidden_dim)\n",
    "            #rnn.hidden2 = rnn.init_hidden(rnn.hidden_dim2)\n",
    "            #idxs = permutation[i:i+BATCH_SIZE]\n",
    "        \n",
    "            out = rnn(inputs[i:i+BATCH_SIZE].view(SEQ_LEN,BATCH_SIZE,-1))    \n",
    "\n",
    "            loss = criterion(out,targets[i:i+BATCH_SIZE].view(-1).long())\n",
    "            losses = np.append(losses,loss.data[0])\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        print(\"Epoch {}/{}\\nLoss: {:.2f}\".format(epoch+1,epochs,losses.mean()))\n",
    "        print(\"=\"*15)\n",
    "    \n",
    "    \n",
    "    rnn.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Loss: 2.82\n",
      "===============\n",
      "Epoch 2/100\n",
      "Loss: 2.69\n",
      "===============\n",
      "Epoch 3/100\n",
      "Loss: 2.64\n",
      "===============\n",
      "Epoch 4/100\n",
      "Loss: 2.61\n",
      "===============\n",
      "Epoch 5/100\n",
      "Loss: 2.60\n",
      "===============\n",
      "Epoch 6/100\n",
      "Loss: 2.59\n",
      "===============\n",
      "Epoch 7/100\n",
      "Loss: 2.59\n",
      "===============\n",
      "Epoch 8/100\n",
      "Loss: 2.58\n",
      "===============\n",
      "Epoch 9/100\n",
      "Loss: 2.58\n",
      "===============\n",
      "Epoch 10/100\n",
      "Loss: 2.57\n",
      "===============\n",
      "Epoch 11/100\n",
      "Loss: 2.55\n",
      "===============\n",
      "Epoch 12/100\n",
      "Loss: 2.54\n",
      "===============\n",
      "Epoch 13/100\n",
      "Loss: 2.54\n",
      "===============\n",
      "Epoch 14/100\n",
      "Loss: 2.53\n",
      "===============\n",
      "Epoch 15/100\n",
      "Loss: 2.52\n",
      "===============\n",
      "Epoch 16/100\n",
      "Loss: 2.52\n",
      "===============\n",
      "Epoch 17/100\n",
      "Loss: 2.51\n",
      "===============\n",
      "Epoch 18/100\n",
      "Loss: 2.51\n",
      "===============\n",
      "Epoch 19/100\n",
      "Loss: 2.50\n",
      "===============\n",
      "Epoch 20/100\n",
      "Loss: 2.50\n",
      "===============\n",
      "Epoch 21/100\n",
      "Loss: 2.50\n",
      "===============\n",
      "Epoch 22/100\n",
      "Loss: 2.50\n",
      "===============\n",
      "Epoch 23/100\n",
      "Loss: 2.49\n",
      "===============\n",
      "Epoch 24/100\n",
      "Loss: 2.49\n",
      "===============\n",
      "Epoch 25/100\n",
      "Loss: 2.49\n",
      "===============\n",
      "Epoch 26/100\n",
      "Loss: 2.49\n",
      "===============\n",
      "Epoch 27/100\n",
      "Loss: 2.49\n",
      "===============\n",
      "Epoch 28/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 29/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 30/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 31/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 32/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 33/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 34/100\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 35/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 36/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 37/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 38/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 39/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 40/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 41/100\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 42/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 43/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 44/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 45/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 46/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 47/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 48/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 49/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 50/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 51/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 52/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 53/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 54/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 55/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 56/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 57/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 58/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 59/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 60/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 61/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 62/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 63/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 64/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 65/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 66/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 67/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 68/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 69/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 70/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 71/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 72/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 73/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 74/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 75/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 76/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 77/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 78/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 79/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 80/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 81/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 82/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 83/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 84/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 85/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 86/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 87/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 88/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 89/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 90/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 91/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 92/100\n",
      "Loss: 2.44\n",
      "===============\n",
      "Epoch 93/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 94/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 95/100\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 96/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 97/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 98/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 99/100\n",
      "Loss: 2.45\n",
      "===============\n",
      "Epoch 100/100\n",
      "Loss: 2.45\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "chunks = 500000\n",
    "\n",
    "#for i in range(0,len(data),chunks):\n",
    "train(data[:0+chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nih e touieuitlttaen \n",
      "\n",
      " e tifh te eaoolei eneetn h i itf e  tst nte olti a acdnlnln  hu t e h renthetg teraofaenin  irl tn tohutlg h\"t  tlcedtrtoriaeten tntel itn ine uen edal t   tesei of aleo t nt ouotte l n   oteiil ttnet uetilt  i onottdthifneetef n dtdo.t enthen  er ihdnoeteeenn n ttc tn et   t no e n goiinantetheei honei oht o n ieneilnehtuo o nio onhltlen eh ut eo et  iot iunlinitsgrtldoi tiniitfer tleo laeoh eono tgiteleon tneaernen eoheoau tntit esio  neteret sf t ad lo htaeneo  o onleo n o io ut nt e ethedo  erer lf hrat e  tieu i   oyniohtc out e tedle an  airl  tf tntdl tino or n\"\n",
      " ie   ueiti  taleratl  eroaletiedonh  niteen , noif o ertauiineai.ee ie di tghuseiirie,tn ,oasorttina h eat en hnreatl eledeoe h e ,terela lt n an o   d h oeneesetyhil  l e t eneii ut  t   ne ae  iei u on ae eoet    heiorto n  o lecnea\n",
      "tat tn tn h uoa enehee et en tei irtit  ooenthnt llthenelilora ueen  lgi ut t enaeaaosy  aedttedgonh oc sanroo hnd ien,  eehrh e  ontoednetuetni toanithicee d tesslhteeh,e diteeliei ht  a t i trelt lei t o  rean anien  tiniaennao  ohu r i  huoahrenenhlo ioeettean ahet her  irgtueoi r  edo etc n hn  tee o nhntt eanhln aeor  hhueitotorera on h uou  t ye thneetneeuoelnieohen tanlog tnsisoere dceyeaac tntenelhonth dilht inot eu ne   hileey,triatotndttfooco ulianthence tso  c  tierohha teero  t eoti  ol hr  ,hort ldeo le  ete eni c hhaenarr etedt ert hn  aoel te aiule t e tin  e .roh nittuodttel odroieno   oi rhet ltel tane.\n",
      "ae.as ue  eltn thorteecdtto hdett o ont cd riouite2s, eet thit e o lr oh teithidrhitgea  tedna nt ne leehtdi ei ooof lteu t elen neof t onacetent nti ean ilauittndce ohn   teirecid elot euirhnient,.trfo  enaatee alo n tin  i te uhehntiteilshauen  tuetd en  itcg haahn ei or  t ret i tsn ie oa   ha e eedoo fie h e h nni tu trer d ehtestnnee tift ed ten ton  cd in a entnh tu oettd   ohuhoritet  oniirite teng .oiredi eaitee in ei d tc  detsic  tl t e ifeiaone.tueto  tent ntuete r   a e teau n iueaeriie lti heno rdoiele iol n el ta euoa e ht eeri ooun  trneyt ethlo r  . od hino dailoentieirr  eterystita ntt    ir hu adigdlhnetl anu ot tdn e a  nenete o t areou  tu t e tnl  eo,ne oe neotau heeltee hslo useteee n  tione h f irettaae a ier t erat e a role e tet ut ento a lhe ttaue ilditlilsdo t  ed   iee toior  i d ninttue tono ng id eantna  t ttu  de ieneho goltol tn e eied le  reoinldo t onhteu  te ele hyn lrdtuoo i torne oale,ter tu.i erieet  hd i  ht ettuta  rhtoaneeie thfh lo ieiant nd t n t  ieu ttoentouai e od o.in o na tadina d ttroi l uet uaioe lthoeereo  shaethiraoaetho n  die ttd tl eec d a fh   inah tteiheuert ed th d  e \n",
      "ttate t erli t erettelcetn  i   hs ue tinahe aicn hntiole,ld ih n u do tlt l onl e r o ut uiua oieniooranihitshet ualt en nrho  tt hlt touet ineereau  haued tat i  iledi  oonri c  ehne ilualith he intneh eetoilu  ugiteraetu o e aen lentet eteitlthneo ed   o tietlltel  oteeenh   ieih e ilan sal hnea lrfeutnel t  is    tun th nent netu hor t enaeash nl  tdla r n ey teleeentehtld teile ash riitariainn ilh iuet entne t utnee et  d  al aon,he   oeueel tl  ttt,sit ut n  o e tor.i ceytnio aene  io rtortoutrelatd h e tanatudneteldiieleat idiean   eretodoot otleainetouaore\"i el atltordi tnt neelin tnetue gne tn  oesit en ont,t tiuat lal t ty,aauae ten refg ,te,adt e th ein ioefertheuidi teue d,hedeet a.teneth l aerne tet e iedn ntttuoa e tnndriieer ter  e it  tne,t eniin ouc i ooouon tihh neuen h aintoten  nt ioeeciotnt rsge ten oo on eynats aecain oneos n ehedofentoe  seul tr t   teu ht la e lentehnien ir netnta tfiedete t  teoah thhee oleidg inet atee ueae eer ecl  trfhoenitnaero  hi le siunte a  idttfe,tane toentirafehet  \n",
      " \n",
      " edtae e ynnottl onterl ato hete   hen toi n tn tn hehoeteehne trofsieno olitisetngee tin tdilrstueen ney iu aan  eea es nd tneii necgh toni ele t ri o not i ote i iett tn  u eele in  atti  tosug aueaide euntaih  tir non ihf hotan itr itonotoc atti tereiond tis  a e ia,ua f tre dtte r rothltion ieoie seteltnenen ea eceetele tl ieerect u sot at tee\n",
      "tn ,i enotg an oiuonann ,a erhltenauitent tel si tte ledotelihnt  ,ho oen toerl eoi tidouo hiineec daenet eltedol til  aouee e nn hhu t etneei  ietoen tothde.atl i ntnngin  hhec tlna netel aee io  oo eodereten  hniine ie e   e aroeennesn i \n",
      "\n",
      "nofert ilt ttloti d t ndeuee eoneathr hriontteniu h e,l itese rinnehlgort ihoiul etnhtinar   tc gn elet slelhnutt uaeortrta e.the tett drhu tn  agl le \"oset   orot utanhrttne hr aiuh  e hu a iaeueae la  dtint ttte e e e treolaiuaet eteaou e.ili  ld dia rde terle ohu o te .tnettt r oeorhoe te erhod h l inon nottoretgn nudt ldiilot nane n   n ircoened t  or iuh uer  alt edue hn iidaeten tetatel re  aoi  dleagfhtllts  rn orlet ntiedh ens hu oeedahenth tntlleo oth eonhe  oe n thlieur   iida eu  tet h eteeliiteui ihn etis,nedg ni telet ,aelitnii reoeo ht  lanetulete iu hhe aheuy itenhetore tlaai i i   e t  e,te n ei li\"ao al i edt c na ael tlier l atriidehnfg tnihtte ee odn inehon leesonetdeoionnge\"\n",
      "\n",
      "ieni ec cgehreefaend n  h  t tu eno t nhitn t, to  elt  te au teil  oen  td t tneteau lituiord  ye ehi hiti a ehierdsan oier  tgrteen d ilahutenh ano t i.er.h anoie  e  ahnte tha hrn o fen r iten te n nitie enlelohle  tn teedetah o tenn a nong  teis   eo fhneul,i e ire u  ftetann t  auoant uo ti trda do  u ieen tendl h clht n iecyn   tn   t tuh  eeou  iiiidtene  tn thanithltt dr,r t e   tho o noan  tea roa uenif oa t\"thnah  hoytiera te   ilouyl d e ito tni\n",
      "han eaiodh  if  ao gnianenerhneon ie i et ue te.iilt nie  ieeeoon  otee h   aa rtauaenrd  ted\"ehniou  tn ar\"t \"ho   tite  d oudoene ihsiee   h  oe  areeei teoni eadet dl iineeaedr ihhhdolil ldetgf reot  intuothe oireenn  itn t arin lei e ne tr i nirde g ta i u e t fedarrae  etes ninue h iuet e   eileneaeuu  tttetot neldl h euite ioe otiu o  lenu   td er htir iteiltyne aedolenntitld,aie ti tetoneo t  af oeu he reieong ihe d atuin  td td ,h n ,  telee hten ai  ilelda et uael hnh etc tro oei eu   nhrlhneoode eenoueuiceneo ns e  luu etleh ire eoiaeno  iuulietn ,ienhtioleen. eo c eoioneaaneh    l  inlofel a inten nd  oeneh eiireeloroa uturieli    rltleee  htnt irn l dr itrrt ntehruei huiiengnntn t a i eel itf tet a\"io i trl  hten otu a iroosteclnloindl eote tiu  tdtede tono cen n i leritsitnll neii ontu tedehoaoi  hli e tlt engioeneeh ntdeood h  hesre  coane ollen  oreele tenitenee ,tneneteaue t  itre deeaa noc  toedt n hlynot t teni e en  ete tait nhi ed td h  ,ngei t,teetdtehrelte.hen teiito art   t telt  te te tndeiut,t nyert on iti.ingelraid ooreuhne  t s tnht e oenr   i o   au tn a lhdie d a n l iiiette e ootu hh oenohh o oen iel i ee ou   te l ic it eneeoselatoi ent i , thed ,i eanuetuite  ce tde henon  inafeltl i ediii teti he. telslot tellah erotn hon nonf  h n hfet ntontaaderenrlho,unhetrei guhthrgnl tllia elnot eh   inono.ihn ehet tieneho l, nnotn at d tl   hnu o eeuneeah  t  ant ntheuen  trd tu oteennitto ttntonghoortnan tuetenlnon  itedtonee  tecei ertcli ainanee,oa e eirt cocyr holoi e teons,t u oanot ilatdinse heueereenreainaioen ec    t not tonen .atcn  a\" ttleleoan  en  taniet n  ianih n  htelh noo eue  t in st eihoeuaheuosnlaeniten sh, and oen o retoguitau lehifat erietiiunrt  irti nen tr nein o ea u t  t ed toodo euec   hntneieele tn enuntt utee aleilit  hougi eatolie hel tefltohenatnst ti ene  t\"\n",
      " udo e huaneetudh  toe oin c otan tei teeu  h uo  aann iene oh e ahhleeou tneeslel.an o  a uotoertend  latu tes ho hhtoneee ueu ninito u i    ttein   onr tt a  nhos ouee eeneitr h  ten  a enn  oent euortnghnhoe te.io ol  r aen t ei ethet t  d o tef dtrd hd sr rh oh  eln e  tn  te,tt eynieanin tt ertienae    n te oe ct  t ratoo tertellatus t ed eer  tu tee t nee aaneeyl  ncd eneiho eilt ian  tisohnee  feu  .tentgiiinaert ete  fctire oeceetsile e  ,to hr ai t  nnteo nhet nioleae ane hn  te efeoin h ien tohthn tf hdieteoli nitey oeflhsetod eh  teeoini torydte,ettniaeciti ui hhute ain nfe ide tteoaitnieny i n e to n eoon ii herene dete naau oeni\"t  iinsooa nehn tg   tn  r  ooiu tnt r  tneydh a s   tt idt i reorith  r  lioninn sad  a e neeadidaue  iiie tuefn n tt ,edotnei edthu er aen nidt a n  htn h neire  niiedotdu iu . a\n",
      "s t l tiilto enot rta hn a en taloaen ehonedi eteia r dee,oaen.  tsardoniie len ttileue irdiieneoeelte a el it   elieh\n"
     ]
    }
   ],
   "source": [
    "string = ''  \n",
    "temperature = 0.5\n",
    "inputs,targets = prepare_seq(data[:10000],drop_last=True)\n",
    "\n",
    "#idxs = permutation[i:i+BATCH_SIZE]\n",
    "rnn.train(False)\n",
    "for i in range(0,inputs.size()[0],BATCH_SIZE):\n",
    "    \n",
    "    out = rnn(inputs[i:i+BATCH_SIZE].view(SEQ_LEN,BATCH_SIZE,-1))\n",
    "    _ ,ix = out.topk(5)\n",
    "    \n",
    "    for j in range(ix.shape[0]):\n",
    "    \n",
    "        t = random.random()\n",
    "\n",
    "        if t < temperature:\n",
    "            selection = random.randint(1,ix.shape[1]-1)\n",
    "        else:\n",
    "            selection = 0\n",
    "        \n",
    "        string += ix_to_char[ix[j,selection].data[0]]\n",
    "\n",
    "print(string)       \n",
    "#print(string,file=open('texts/output.txt','w'))\n",
    "    \n",
    "    #with open('texts/output.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "        #myfile.writelines(' '.join(lyrics))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
