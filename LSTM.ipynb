{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('texts/necronomicon.txt','r').read()\n",
    "alphabet = set(data.lower())\n",
    "alphabet_size = len(data)\n",
    "\n",
    "ix_to_char = {k:v for k,v in enumerate(alphabet)}\n",
    "char_to_ix = {k:v for v,k in enumerate(alphabet)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, alphabet_size, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(alphabet_size, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, alphabet_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, text):\n",
    "        lstm_out, self.hidden = self.lstm(text, self.hidden)\n",
    "        out = self.fc(lstm_out.view(len(text), -1))\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        return (autograd.Variable(torch.zeros(NUM_LAYERS,BATCH_SIZE,self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(NUM_LAYERS, BATCH_SIZE, self.hidden_dim)))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 64\n",
    "SEQ_LEN = 64\n",
    "\n",
    "rnn= LSTM(alphabet_size,HIDDEN_DIM)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to prepare sequences\n",
    "# creating len(data)/SEQ_LEN number of vectors of SEQ_LEN lenght\n",
    "\n",
    "\n",
    "def prepare_seq():\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " Loss: 9.35\n",
      "===============\n",
      "Epoch 2/100\n",
      " Loss: 9.19\n",
      "===============\n",
      "Epoch 3/100\n",
      " Loss: 8.99\n",
      "===============\n",
      "Epoch 4/100\n",
      " Loss: 8.68\n",
      "===============\n",
      "Epoch 5/100\n",
      " Loss: 8.07\n",
      "===============\n",
      "Epoch 6/100\n",
      " Loss: 7.04\n",
      "===============\n",
      "Epoch 7/100\n",
      " Loss: 6.26\n",
      "===============\n",
      "Epoch 8/100\n",
      " Loss: 5.78\n",
      "===============\n",
      "Epoch 9/100\n",
      " Loss: 5.49\n",
      "===============\n",
      "Epoch 10/100\n",
      " Loss: 5.28\n",
      "===============\n",
      "Epoch 11/100\n",
      " Loss: 5.16\n",
      "===============\n",
      "Epoch 12/100\n",
      " Loss: 5.11\n",
      "===============\n",
      "Epoch 13/100\n",
      " Loss: 5.09\n",
      "===============\n",
      "Epoch 14/100\n",
      " Loss: 5.08\n",
      "===============\n",
      "Epoch 15/100\n",
      " Loss: 5.04\n",
      "===============\n",
      "Epoch 16/100\n",
      " Loss: 4.98\n",
      "===============\n",
      "Epoch 17/100\n",
      " Loss: 4.91\n",
      "===============\n",
      "Epoch 18/100\n",
      " Loss: 4.82\n",
      "===============\n",
      "Epoch 19/100\n",
      " Loss: 4.73\n",
      "===============\n",
      "Epoch 20/100\n",
      " Loss: 4.65\n",
      "===============\n",
      "Epoch 21/100\n",
      " Loss: 4.57\n",
      "===============\n",
      "Epoch 22/100\n",
      " Loss: 4.49\n",
      "===============\n",
      "Epoch 23/100\n",
      " Loss: 4.39\n",
      "===============\n",
      "Epoch 24/100\n",
      " Loss: 4.29\n",
      "===============\n",
      "Epoch 25/100\n",
      " Loss: 4.19\n",
      "===============\n",
      "Epoch 26/100\n",
      " Loss: 4.09\n",
      "===============\n",
      "Epoch 27/100\n",
      " Loss: 3.99\n",
      "===============\n",
      "Epoch 28/100\n",
      " Loss: 3.89\n",
      "===============\n",
      "Epoch 29/100\n",
      " Loss: 3.78\n",
      "===============\n",
      "Epoch 30/100\n",
      " Loss: 3.67\n",
      "===============\n",
      "Epoch 31/100\n",
      " Loss: 3.56\n",
      "===============\n",
      "Epoch 32/100\n",
      " Loss: 3.45\n",
      "===============\n",
      "Epoch 33/100\n",
      " Loss: 3.33\n",
      "===============\n",
      "Epoch 34/100\n",
      " Loss: 3.21\n",
      "===============\n",
      "Epoch 35/100\n",
      " Loss: 3.10\n",
      "===============\n",
      "Epoch 36/100\n",
      " Loss: 2.98\n",
      "===============\n",
      "Epoch 37/100\n",
      " Loss: 2.86\n",
      "===============\n",
      "Epoch 38/100\n",
      " Loss: 2.74\n",
      "===============\n",
      "Epoch 39/100\n",
      " Loss: 2.62\n",
      "===============\n",
      "Epoch 40/100\n",
      " Loss: 2.51\n",
      "===============\n",
      "Epoch 41/100\n",
      " Loss: 2.39\n",
      "===============\n",
      "Epoch 42/100\n",
      " Loss: 2.28\n",
      "===============\n",
      "Epoch 43/100\n",
      " Loss: 2.17\n",
      "===============\n",
      "Epoch 44/100\n",
      " Loss: 2.06\n",
      "===============\n",
      "Epoch 45/100\n",
      " Loss: 1.96\n",
      "===============\n",
      "Epoch 46/100\n",
      " Loss: 1.86\n",
      "===============\n",
      "Epoch 47/100\n",
      " Loss: 1.76\n",
      "===============\n",
      "Epoch 48/100\n",
      " Loss: 1.66\n",
      "===============\n",
      "Epoch 49/100\n",
      " Loss: 1.57\n",
      "===============\n",
      "Epoch 50/100\n",
      " Loss: 1.49\n",
      "===============\n",
      "Epoch 51/100\n",
      " Loss: 1.41\n",
      "===============\n",
      "Epoch 52/100\n",
      " Loss: 1.33\n",
      "===============\n",
      "Epoch 53/100\n",
      " Loss: 1.26\n",
      "===============\n",
      "Epoch 54/100\n",
      " Loss: 1.19\n",
      "===============\n",
      "Epoch 55/100\n",
      " Loss: 1.13\n",
      "===============\n",
      "Epoch 56/100\n",
      " Loss: 1.07\n",
      "===============\n",
      "Epoch 57/100\n",
      " Loss: 1.02\n",
      "===============\n",
      "Epoch 58/100\n",
      " Loss: 0.97\n",
      "===============\n",
      "Epoch 59/100\n",
      " Loss: 0.92\n",
      "===============\n",
      "Epoch 60/100\n",
      " Loss: 0.88\n",
      "===============\n",
      "Epoch 61/100\n",
      " Loss: 0.84\n",
      "===============\n",
      "Epoch 62/100\n",
      " Loss: 0.80\n",
      "===============\n",
      "Epoch 63/100\n",
      " Loss: 0.76\n",
      "===============\n",
      "Epoch 64/100\n",
      " Loss: 0.73\n",
      "===============\n",
      "Epoch 65/100\n",
      " Loss: 0.70\n",
      "===============\n",
      "Epoch 66/100\n",
      " Loss: 0.68\n",
      "===============\n",
      "Epoch 67/100\n",
      " Loss: 0.65\n",
      "===============\n",
      "Epoch 68/100\n",
      " Loss: 0.63\n",
      "===============\n",
      "Epoch 69/100\n",
      " Loss: 0.61\n",
      "===============\n",
      "Epoch 70/100\n",
      " Loss: 0.59\n",
      "===============\n",
      "Epoch 71/100\n",
      " Loss: 0.57\n",
      "===============\n",
      "Epoch 72/100\n",
      " Loss: 0.55\n",
      "===============\n",
      "Epoch 73/100\n",
      " Loss: 0.53\n",
      "===============\n",
      "Epoch 74/100\n",
      " Loss: 0.51\n",
      "===============\n",
      "Epoch 75/100\n",
      " Loss: 0.50\n",
      "===============\n",
      "Epoch 76/100\n",
      " Loss: 0.49\n",
      "===============\n",
      "Epoch 77/100\n",
      " Loss: 0.47\n",
      "===============\n",
      "Epoch 78/100\n",
      " Loss: 0.46\n",
      "===============\n",
      "Epoch 79/100\n",
      " Loss: 0.45\n",
      "===============\n",
      "Epoch 80/100\n",
      " Loss: 0.43\n",
      "===============\n",
      "Epoch 81/100\n",
      " Loss: 0.42\n",
      "===============\n",
      "Epoch 82/100\n",
      " Loss: 0.41\n",
      "===============\n",
      "Epoch 83/100\n",
      " Loss: 0.40\n",
      "===============\n",
      "Epoch 84/100\n",
      " Loss: 0.39\n",
      "===============\n",
      "Epoch 85/100\n",
      " Loss: 0.38\n",
      "===============\n",
      "Epoch 86/100\n",
      " Loss: 0.37\n",
      "===============\n",
      "Epoch 87/100\n",
      " Loss: 0.36\n",
      "===============\n",
      "Epoch 88/100\n",
      " Loss: 0.36\n",
      "===============\n",
      "Epoch 89/100\n",
      " Loss: 0.35\n",
      "===============\n",
      "Epoch 90/100\n",
      " Loss: 0.34\n",
      "===============\n",
      "Epoch 91/100\n",
      " Loss: 0.33\n",
      "===============\n",
      "Epoch 92/100\n",
      " Loss: 0.32\n",
      "===============\n",
      "Epoch 93/100\n",
      " Loss: 0.32\n",
      "===============\n",
      "Epoch 94/100\n",
      " Loss: 0.31\n",
      "===============\n",
      "Epoch 95/100\n",
      " Loss: 0.30\n",
      "===============\n",
      "Epoch 96/100\n",
      " Loss: 0.30\n",
      "===============\n",
      "Epoch 97/100\n",
      " Loss: 0.29\n",
      "===============\n",
      "Epoch 98/100\n",
      " Loss: 0.28\n",
      "===============\n",
      "Epoch 99/100\n",
      " Loss: 0.28\n",
      "===============\n",
      "Epoch 100/100\n",
      " Loss: 0.27\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    losses = np.array()\n",
    "    \n",
    "    # sample data from random vectors \n",
    "        \n",
    "    permutation = torch.randperm(len(data))\n",
    "    rnn.zero_grad()\n",
    "    rnn.hidden = model.init_hidden()\n",
    "    \n",
    "    for i in range(0,len(data),BATCH_SIZE):\n",
    "\n",
    "        index = permutation[i:i+BATCH_SIZE]\n",
    "        out = rnn(text_ix2[index])    \n",
    "\n",
    "        loss = criterion(out,target_ix2)\n",
    "        losses.append(loss.data[0])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    print(\"Epoch {}/{}\\n Loss: {:.2f}\".format(epoch+1,epochs,losses.mean()))\n",
    "    print(\"=\"*15)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(text_ix)\n",
    "_ ,ix = out.max(1)\n",
    "        \n",
    "lyrics = [ix_to_word[w.data[0]] for w in ix]\n",
    "\n",
    "with open('texts/output.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.writelines(' '.join(lyrics))\n",
    "    myfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(prepare_sequence(['black'],word_to_ix))\n",
    "_ ,ix = out.max(1)\n",
    "\n",
    "idxs = []\n",
    "\n",
    "for _ in range(5000):\n",
    "\n",
    "    out = model(ix)\n",
    "    _ ,ix = out.max(1)\n",
    "    \n",
    "    idxs.append(ix)\n",
    "    \n",
    "        \n",
    "lyrics = [ix_to_word[w.data[0]] for w in idxs]\n",
    "\n",
    "with open('texts/output.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.writelines(' '.join(lyrics))\n",
    "    myfile.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
