{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DTYPE = torch.cuda.FloatTensor\n",
    "else:\n",
    "    DTYPE = torch.FloatTensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('texts/Lovecraft.txt','r').read()\n",
    "alphabet = set(text)\n",
    "\n",
    "ix_to_char = {k:v for k,v in enumerate(alphabet)}\n",
    "char_to_ix = {k:v for v,k in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 512\n",
    "SEQ_LEN = 65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_tensor(sequence):\n",
    "    tensor = torch.zeros(len(sequence),len(alphabet)).type(DTYPE)\n",
    "    for i, c in enumerate(sequence):\n",
    "        tensor[i][char_to_ix[c]] = 1\n",
    "    return tensor.view(BATCH_SIZE,SEQ_LEN,len(alphabet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtLoader(data.Dataset):\n",
    "    \n",
    "    def __init__(self,text):\n",
    "        super(TxtLoader,self).__init__()\n",
    "        self.data = text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,alphabet_size, hidden_dim, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.i2h = nn.Linear(alphabet_size,hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim,hidden_dim,NUM_LAYERS,batch_first=True,dropout=True)\n",
    "        self.h2O = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "       \n",
    "        return (autograd.Variable(torch.zeros(NUM_LAYERS, BATCH_SIZE, self.hidden_dim).type(DTYPE)),\n",
    "                autograd.Variable(torch.zeros(NUM_LAYERS, BATCH_SIZE, self.hidden_dim).type(DTYPE)))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        out = self.i2h(sequence)\n",
    "        lstm_out, self.hidden = self.lstm(out.view(BATCH_SIZE,SEQ_LEN-1,-1),self.hidden)\n",
    "        out = self.h2O(lstm_out.contiguous().view(-1,self.hidden_dim))\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def gen_text(self, batch,t=None):\n",
    "            \n",
    "        inputs = autograd.Variable(sequence_to_tensor(batch))\n",
    "        idxs = torch.zeros(inputs.data.size())\n",
    "        out = self(inputs[:,:-1,:])\n",
    "\n",
    "        if t != None:\n",
    "            soft_out = F.softmax(out/t,dim=1)\n",
    "            \n",
    "            for i in range(soft_out.size()[0]):\n",
    "                idxs[i] = np.random.choice(soft_out.size()[1],p=soft_out.data.numpy()[i])\n",
    "                \n",
    "        else:\n",
    "            idxs = out.max(1)[1].data\n",
    "\n",
    "        \n",
    "        return out,idxs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LSTM(len(alphabet),HIDDEN_DIM,len(alphabet)).type(DTYPE)\n",
    "optimizer = optim.Adam(rnn.parameters(),lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    rnn.train(True)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        losses = np.array([])\n",
    "        \n",
    "        for batch in data_loader:\n",
    "\n",
    "            rnn.zero_grad()\n",
    "            rnn.hidden = rnn.init_hidden()\n",
    "            \n",
    "            inputs = autograd.Variable(sequence_to_tensor(batch))\n",
    "            \n",
    "                        \n",
    "            out = rnn(inputs[:,:-1,:])\n",
    "                        \n",
    "            _,target = inputs[:,1:,:].topk(1)\n",
    "            \n",
    "            \n",
    "            loss = criterion(out.view(-1,len(alphabet)),target.view(-1))\n",
    "            losses = np.append(losses,loss.data[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if losses.mean() < best_loss:\n",
    "            best_loss = losses.mean()\n",
    "            best_wts = rnn.state_dict()\n",
    "            \n",
    "\n",
    "        print(\"Epoch {}/{}\\nLoss: {:.2f}\".format(epoch+1,epochs,losses.mean()))\n",
    "        print(\"=\"*15)\n",
    "        \n",
    "    \n",
    "    return best_wts\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TxtLoader(text[:10000])\n",
    "loader = data.DataLoader(dataset,batch_size=BATCH_SIZE*SEQ_LEN,drop_last=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "Loss: 4.37\n",
      "===============\n",
      "Epoch 2/300\n",
      "Loss: 3.28\n",
      "===============\n",
      "Epoch 3/300\n",
      "Loss: 3.78\n",
      "===============\n",
      "Epoch 4/300\n",
      "Loss: 3.67\n",
      "===============\n",
      "Epoch 5/300\n",
      "Loss: 3.47\n",
      "===============\n",
      "Epoch 6/300\n",
      "Loss: 3.31\n",
      "===============\n",
      "Epoch 7/300\n",
      "Loss: 3.19\n",
      "===============\n",
      "Epoch 8/300\n",
      "Loss: 3.10\n",
      "===============\n",
      "Epoch 9/300\n",
      "Loss: 3.02\n",
      "===============\n",
      "Epoch 10/300\n",
      "Loss: 2.98\n",
      "===============\n",
      "Epoch 11/300\n",
      "Loss: 2.99\n",
      "===============\n",
      "Epoch 12/300\n",
      "Loss: 2.95\n",
      "===============\n",
      "Epoch 13/300\n",
      "Loss: 2.88\n",
      "===============\n",
      "Epoch 14/300\n",
      "Loss: 2.83\n",
      "===============\n",
      "Epoch 15/300\n",
      "Loss: 2.79\n",
      "===============\n",
      "Epoch 16/300\n",
      "Loss: 2.75\n",
      "===============\n",
      "Epoch 17/300\n",
      "Loss: 2.70\n",
      "===============\n",
      "Epoch 18/300\n",
      "Loss: 2.67\n",
      "===============\n",
      "Epoch 19/300\n",
      "Loss: 2.64\n",
      "===============\n",
      "Epoch 20/300\n",
      "Loss: 2.60\n",
      "===============\n",
      "Epoch 21/300\n",
      "Loss: 2.68\n",
      "===============\n",
      "Epoch 22/300\n",
      "Loss: 2.58\n",
      "===============\n",
      "Epoch 23/300\n",
      "Loss: 2.59\n",
      "===============\n",
      "Epoch 24/300\n",
      "Loss: 2.59\n",
      "===============\n",
      "Epoch 25/300\n",
      "Loss: 2.58\n",
      "===============\n",
      "Epoch 26/300\n",
      "Loss: 2.57\n",
      "===============\n",
      "Epoch 27/300\n",
      "Loss: 2.55\n",
      "===============\n",
      "Epoch 28/300\n",
      "Loss: 2.54\n",
      "===============\n",
      "Epoch 29/300\n",
      "Loss: 2.52\n",
      "===============\n",
      "Epoch 30/300\n",
      "Loss: 2.51\n",
      "===============\n",
      "Epoch 31/300\n",
      "Loss: 2.49\n",
      "===============\n",
      "Epoch 32/300\n",
      "Loss: 2.48\n",
      "===============\n",
      "Epoch 33/300\n",
      "Loss: 2.47\n",
      "===============\n",
      "Epoch 34/300\n",
      "Loss: 2.46\n",
      "===============\n",
      "Epoch 35/300\n",
      "Loss: 2.44\n",
      "===============\n",
      "Epoch 36/300\n",
      "Loss: 2.43\n",
      "===============\n",
      "Epoch 37/300\n",
      "Loss: 2.41\n",
      "===============\n",
      "Epoch 38/300\n",
      "Loss: 2.40\n",
      "===============\n",
      "Epoch 39/300\n",
      "Loss: 2.39\n",
      "===============\n",
      "Epoch 40/300\n",
      "Loss: 2.38\n",
      "===============\n",
      "Epoch 41/300\n",
      "Loss: 2.37\n",
      "===============\n",
      "Epoch 42/300\n",
      "Loss: 2.36\n",
      "===============\n",
      "Epoch 43/300\n",
      "Loss: 2.35\n",
      "===============\n",
      "Epoch 44/300\n",
      "Loss: 2.34\n",
      "===============\n",
      "Epoch 45/300\n",
      "Loss: 2.33\n",
      "===============\n",
      "Epoch 46/300\n",
      "Loss: 2.32\n",
      "===============\n",
      "Epoch 47/300\n",
      "Loss: 2.31\n",
      "===============\n",
      "Epoch 48/300\n",
      "Loss: 2.30\n",
      "===============\n",
      "Epoch 49/300\n",
      "Loss: 2.29\n",
      "===============\n",
      "Epoch 50/300\n",
      "Loss: 2.28\n",
      "===============\n",
      "Epoch 51/300\n",
      "Loss: 2.27\n",
      "===============\n",
      "Epoch 52/300\n",
      "Loss: 2.26\n",
      "===============\n",
      "Epoch 53/300\n",
      "Loss: 2.25\n",
      "===============\n",
      "Epoch 54/300\n",
      "Loss: 2.24\n",
      "===============\n",
      "Epoch 55/300\n",
      "Loss: 2.23\n",
      "===============\n",
      "Epoch 56/300\n",
      "Loss: 2.23\n",
      "===============\n",
      "Epoch 57/300\n",
      "Loss: 2.22\n",
      "===============\n",
      "Epoch 58/300\n",
      "Loss: 2.21\n",
      "===============\n",
      "Epoch 59/300\n",
      "Loss: 2.20\n",
      "===============\n",
      "Epoch 60/300\n",
      "Loss: 2.19\n",
      "===============\n",
      "Epoch 61/300\n",
      "Loss: 2.18\n",
      "===============\n",
      "Epoch 62/300\n",
      "Loss: 2.17\n",
      "===============\n",
      "Epoch 63/300\n",
      "Loss: 2.16\n",
      "===============\n",
      "Epoch 64/300\n",
      "Loss: 2.15\n",
      "===============\n",
      "Epoch 65/300\n",
      "Loss: 2.14\n",
      "===============\n",
      "Epoch 66/300\n",
      "Loss: 2.13\n",
      "===============\n",
      "Epoch 67/300\n",
      "Loss: 2.12\n",
      "===============\n",
      "Epoch 68/300\n",
      "Loss: 2.11\n",
      "===============\n",
      "Epoch 69/300\n",
      "Loss: 2.10\n",
      "===============\n",
      "Epoch 70/300\n",
      "Loss: 2.09\n",
      "===============\n",
      "Epoch 71/300\n",
      "Loss: 2.08\n",
      "===============\n",
      "Epoch 72/300\n",
      "Loss: 2.07\n",
      "===============\n",
      "Epoch 73/300\n",
      "Loss: 2.06\n",
      "===============\n",
      "Epoch 74/300\n",
      "Loss: 2.05\n",
      "===============\n",
      "Epoch 75/300\n",
      "Loss: 2.04\n",
      "===============\n",
      "Epoch 76/300\n",
      "Loss: 2.03\n",
      "===============\n",
      "Epoch 77/300\n",
      "Loss: 2.02\n",
      "===============\n",
      "Epoch 78/300\n",
      "Loss: 2.01\n",
      "===============\n",
      "Epoch 79/300\n",
      "Loss: 2.00\n",
      "===============\n",
      "Epoch 80/300\n",
      "Loss: 1.99\n",
      "===============\n",
      "Epoch 81/300\n",
      "Loss: 1.97\n",
      "===============\n",
      "Epoch 82/300\n",
      "Loss: 1.96\n",
      "===============\n",
      "Epoch 83/300\n",
      "Loss: 1.95\n",
      "===============\n",
      "Epoch 84/300\n",
      "Loss: 1.94\n",
      "===============\n",
      "Epoch 85/300\n",
      "Loss: 1.93\n",
      "===============\n",
      "Epoch 86/300\n",
      "Loss: 1.92\n",
      "===============\n",
      "Epoch 87/300\n",
      "Loss: 1.91\n",
      "===============\n",
      "Epoch 88/300\n",
      "Loss: 1.89\n",
      "===============\n",
      "Epoch 89/300\n",
      "Loss: 1.88\n",
      "===============\n",
      "Epoch 90/300\n",
      "Loss: 1.87\n",
      "===============\n",
      "Epoch 91/300\n",
      "Loss: 1.86\n",
      "===============\n",
      "Epoch 92/300\n",
      "Loss: 1.85\n",
      "===============\n",
      "Epoch 93/300\n",
      "Loss: 1.84\n",
      "===============\n",
      "Epoch 94/300\n",
      "Loss: 1.82\n",
      "===============\n",
      "Epoch 95/300\n",
      "Loss: 1.81\n",
      "===============\n",
      "Epoch 96/300\n",
      "Loss: 1.80\n",
      "===============\n",
      "Epoch 97/300\n",
      "Loss: 1.79\n",
      "===============\n",
      "Epoch 98/300\n",
      "Loss: 1.78\n",
      "===============\n",
      "Epoch 99/300\n",
      "Loss: 1.77\n",
      "===============\n",
      "Epoch 100/300\n",
      "Loss: 1.75\n",
      "===============\n",
      "Epoch 101/300\n",
      "Loss: 1.74\n",
      "===============\n",
      "Epoch 102/300\n",
      "Loss: 1.73\n",
      "===============\n",
      "Epoch 103/300\n",
      "Loss: 1.71\n",
      "===============\n",
      "Epoch 104/300\n",
      "Loss: 1.70\n",
      "===============\n",
      "Epoch 105/300\n",
      "Loss: 1.69\n",
      "===============\n",
      "Epoch 106/300\n",
      "Loss: 1.68\n",
      "===============\n",
      "Epoch 107/300\n",
      "Loss: 1.66\n",
      "===============\n",
      "Epoch 108/300\n",
      "Loss: 1.65\n",
      "===============\n",
      "Epoch 109/300\n",
      "Loss: 1.64\n",
      "===============\n",
      "Epoch 110/300\n",
      "Loss: 1.62\n",
      "===============\n",
      "Epoch 111/300\n",
      "Loss: 1.62\n",
      "===============\n",
      "Epoch 112/300\n",
      "Loss: 1.60\n",
      "===============\n",
      "Epoch 113/300\n",
      "Loss: 1.59\n",
      "===============\n",
      "Epoch 114/300\n",
      "Loss: 1.57\n",
      "===============\n",
      "Epoch 115/300\n",
      "Loss: 1.56\n",
      "===============\n",
      "Epoch 116/300\n",
      "Loss: 1.55\n",
      "===============\n",
      "Epoch 117/300\n",
      "Loss: 1.53\n",
      "===============\n",
      "Epoch 118/300\n",
      "Loss: 1.52\n",
      "===============\n",
      "Epoch 119/300\n",
      "Loss: 1.50\n",
      "===============\n",
      "Epoch 120/300\n",
      "Loss: 1.49\n",
      "===============\n",
      "Epoch 121/300\n",
      "Loss: 1.47\n",
      "===============\n",
      "Epoch 122/300\n",
      "Loss: 1.46\n",
      "===============\n",
      "Epoch 123/300\n",
      "Loss: 1.45\n",
      "===============\n",
      "Epoch 124/300\n",
      "Loss: 1.44\n",
      "===============\n",
      "Epoch 125/300\n",
      "Loss: 1.42\n",
      "===============\n",
      "Epoch 126/300\n",
      "Loss: 1.41\n",
      "===============\n",
      "Epoch 127/300\n",
      "Loss: 1.41\n",
      "===============\n",
      "Epoch 128/300\n",
      "Loss: 1.39\n",
      "===============\n",
      "Epoch 129/300\n",
      "Loss: 1.37\n",
      "===============\n",
      "Epoch 130/300\n",
      "Loss: 1.36\n",
      "===============\n",
      "Epoch 131/300\n",
      "Loss: 1.35\n",
      "===============\n",
      "Epoch 132/300\n",
      "Loss: 1.33\n",
      "===============\n",
      "Epoch 133/300\n",
      "Loss: 1.31\n",
      "===============\n",
      "Epoch 134/300\n",
      "Loss: 1.30\n",
      "===============\n",
      "Epoch 135/300\n",
      "Loss: 1.30\n",
      "===============\n",
      "Epoch 136/300\n",
      "Loss: 1.27\n",
      "===============\n",
      "Epoch 137/300\n",
      "Loss: 1.26\n",
      "===============\n",
      "Epoch 138/300\n",
      "Loss: 1.25\n",
      "===============\n",
      "Epoch 139/300\n",
      "Loss: 1.24\n",
      "===============\n",
      "Epoch 140/300\n",
      "Loss: 1.22\n",
      "===============\n",
      "Epoch 141/300\n",
      "Loss: 1.20\n",
      "===============\n",
      "Epoch 142/300\n",
      "Loss: 1.19\n",
      "===============\n",
      "Epoch 143/300\n",
      "Loss: 1.17\n",
      "===============\n",
      "Epoch 144/300\n",
      "Loss: 1.16\n",
      "===============\n",
      "Epoch 145/300\n",
      "Loss: 1.14\n",
      "===============\n",
      "Epoch 146/300\n",
      "Loss: 1.13\n",
      "===============\n",
      "Epoch 147/300\n",
      "Loss: 1.12\n",
      "===============\n",
      "Epoch 148/300\n",
      "Loss: 1.11\n",
      "===============\n",
      "Epoch 149/300\n",
      "Loss: 1.10\n",
      "===============\n",
      "Epoch 150/300\n",
      "Loss: 1.12\n",
      "===============\n",
      "Epoch 151/300\n",
      "Loss: 1.10\n",
      "===============\n",
      "Epoch 152/300\n",
      "Loss: 1.07\n",
      "===============\n",
      "Epoch 153/300\n",
      "Loss: 1.06\n",
      "===============\n",
      "Epoch 154/300\n",
      "Loss: 1.04\n",
      "===============\n",
      "Epoch 155/300\n",
      "Loss: 1.03\n",
      "===============\n",
      "Epoch 156/300\n",
      "Loss: 1.01\n",
      "===============\n",
      "Epoch 157/300\n",
      "Loss: 1.00\n",
      "===============\n",
      "Epoch 158/300\n",
      "Loss: 0.98\n",
      "===============\n",
      "Epoch 159/300\n",
      "Loss: 0.96\n",
      "===============\n",
      "Epoch 160/300\n",
      "Loss: 0.96\n",
      "===============\n",
      "Epoch 161/300\n",
      "Loss: 0.94\n",
      "===============\n",
      "Epoch 162/300\n",
      "Loss: 0.93\n",
      "===============\n",
      "Epoch 163/300\n",
      "Loss: 0.91\n",
      "===============\n",
      "Epoch 164/300\n",
      "Loss: 0.90\n",
      "===============\n",
      "Epoch 165/300\n",
      "Loss: 0.89\n",
      "===============\n",
      "Epoch 166/300\n",
      "Loss: 0.87\n",
      "===============\n",
      "Epoch 167/300\n",
      "Loss: 0.86\n",
      "===============\n",
      "Epoch 168/300\n",
      "Loss: 0.85\n",
      "===============\n",
      "Epoch 169/300\n",
      "Loss: 0.83\n",
      "===============\n",
      "Epoch 170/300\n",
      "Loss: 0.82\n",
      "===============\n",
      "Epoch 171/300\n",
      "Loss: 0.80\n",
      "===============\n",
      "Epoch 172/300\n",
      "Loss: 0.79\n",
      "===============\n",
      "Epoch 173/300\n",
      "Loss: 0.78\n",
      "===============\n",
      "Epoch 174/300\n",
      "Loss: 0.77\n",
      "===============\n",
      "Epoch 175/300\n",
      "Loss: 0.76\n",
      "===============\n",
      "Epoch 176/300\n",
      "Loss: 0.77\n",
      "===============\n",
      "Epoch 177/300\n",
      "Loss: 0.79\n",
      "===============\n",
      "Epoch 178/300\n",
      "Loss: 0.85\n",
      "===============\n",
      "Epoch 179/300\n",
      "Loss: 0.76\n",
      "===============\n",
      "Epoch 180/300\n",
      "Loss: 0.77\n",
      "===============\n",
      "Epoch 181/300\n",
      "Loss: 0.78\n",
      "===============\n",
      "Epoch 182/300\n",
      "Loss: 0.73\n",
      "===============\n",
      "Epoch 183/300\n",
      "Loss: 0.74\n",
      "===============\n",
      "Epoch 184/300\n",
      "Loss: 0.69\n",
      "===============\n",
      "Epoch 185/300\n",
      "Loss: 0.70\n",
      "===============\n",
      "Epoch 186/300\n",
      "Loss: 0.67\n",
      "===============\n",
      "Epoch 187/300\n",
      "Loss: 0.66\n",
      "===============\n",
      "Epoch 188/300\n",
      "Loss: 0.64\n",
      "===============\n",
      "Epoch 189/300\n",
      "Loss: 0.63\n",
      "===============\n",
      "Epoch 190/300\n",
      "Loss: 0.62\n",
      "===============\n",
      "Epoch 191/300\n",
      "Loss: 0.61\n",
      "===============\n",
      "Epoch 192/300\n",
      "Loss: 0.59\n",
      "===============\n",
      "Epoch 193/300\n",
      "Loss: 0.58\n",
      "===============\n",
      "Epoch 194/300\n",
      "Loss: 0.57\n",
      "===============\n",
      "Epoch 195/300\n",
      "Loss: 0.56\n",
      "===============\n",
      "Epoch 196/300\n",
      "Loss: 0.55\n",
      "===============\n",
      "Epoch 197/300\n",
      "Loss: 0.54\n",
      "===============\n",
      "Epoch 198/300\n",
      "Loss: 0.53\n",
      "===============\n",
      "Epoch 199/300\n",
      "Loss: 0.52\n",
      "===============\n",
      "Epoch 200/300\n",
      "Loss: 0.50\n",
      "===============\n",
      "Epoch 201/300\n",
      "Loss: 0.50\n",
      "===============\n",
      "Epoch 202/300\n",
      "Loss: 0.49\n",
      "===============\n",
      "Epoch 203/300\n",
      "Loss: 0.48\n",
      "===============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/300\n",
      "Loss: 0.47\n",
      "===============\n",
      "Epoch 205/300\n",
      "Loss: 0.46\n",
      "===============\n",
      "Epoch 206/300\n",
      "Loss: 0.46\n",
      "===============\n",
      "Epoch 207/300\n",
      "Loss: 0.46\n",
      "===============\n",
      "Epoch 208/300\n",
      "Loss: 0.46\n",
      "===============\n",
      "Epoch 209/300\n",
      "Loss: 0.44\n",
      "===============\n",
      "Epoch 210/300\n",
      "Loss: 0.42\n",
      "===============\n",
      "Epoch 211/300\n",
      "Loss: 0.42\n",
      "===============\n",
      "Epoch 212/300\n",
      "Loss: 0.41\n",
      "===============\n",
      "Epoch 213/300\n",
      "Loss: 0.40\n",
      "===============\n",
      "Epoch 214/300\n",
      "Loss: 0.39\n",
      "===============\n",
      "Epoch 215/300\n",
      "Loss: 0.38\n",
      "===============\n",
      "Epoch 216/300\n",
      "Loss: 0.37\n",
      "===============\n",
      "Epoch 217/300\n",
      "Loss: 0.36\n",
      "===============\n",
      "Epoch 218/300\n",
      "Loss: 0.36\n",
      "===============\n",
      "Epoch 219/300\n",
      "Loss: 0.35\n",
      "===============\n",
      "Epoch 220/300\n",
      "Loss: 0.34\n",
      "===============\n",
      "Epoch 221/300\n",
      "Loss: 0.33\n",
      "===============\n",
      "Epoch 222/300\n",
      "Loss: 0.32\n",
      "===============\n",
      "Epoch 223/300\n",
      "Loss: 0.31\n",
      "===============\n",
      "Epoch 224/300\n",
      "Loss: 0.31\n",
      "===============\n",
      "Epoch 225/300\n",
      "Loss: 0.30\n",
      "===============\n",
      "Epoch 226/300\n",
      "Loss: 0.29\n",
      "===============\n",
      "Epoch 227/300\n",
      "Loss: 0.29\n",
      "===============\n",
      "Epoch 228/300\n",
      "Loss: 0.28\n",
      "===============\n",
      "Epoch 229/300\n",
      "Loss: 0.28\n",
      "===============\n",
      "Epoch 230/300\n",
      "Loss: 0.27\n",
      "===============\n",
      "Epoch 231/300\n",
      "Loss: 0.27\n",
      "===============\n",
      "Epoch 232/300\n",
      "Loss: 0.28\n",
      "===============\n",
      "Epoch 233/300\n",
      "Loss: 0.29\n",
      "===============\n",
      "Epoch 234/300\n",
      "Loss: 0.27\n",
      "===============\n",
      "Epoch 235/300\n",
      "Loss: 0.26\n",
      "===============\n",
      "Epoch 236/300\n",
      "Loss: 0.26\n",
      "===============\n",
      "Epoch 237/300\n",
      "Loss: 0.25\n",
      "===============\n",
      "Epoch 238/300\n",
      "Loss: 0.24\n",
      "===============\n",
      "Epoch 239/300\n",
      "Loss: 0.23\n",
      "===============\n",
      "Epoch 240/300\n",
      "Loss: 0.22\n",
      "===============\n",
      "Epoch 241/300\n",
      "Loss: 0.22\n",
      "===============\n",
      "Epoch 242/300\n",
      "Loss: 0.21\n",
      "===============\n",
      "Epoch 243/300\n",
      "Loss: 0.21\n",
      "===============\n",
      "Epoch 244/300\n",
      "Loss: 0.20\n",
      "===============\n",
      "Epoch 245/300\n",
      "Loss: 0.20\n",
      "===============\n",
      "Epoch 246/300\n",
      "Loss: 0.19\n",
      "===============\n",
      "Epoch 247/300\n",
      "Loss: 0.19\n",
      "===============\n",
      "Epoch 248/300\n",
      "Loss: 0.18\n",
      "===============\n",
      "Epoch 249/300\n",
      "Loss: 0.18\n",
      "===============\n",
      "Epoch 250/300\n",
      "Loss: 0.17\n",
      "===============\n",
      "Epoch 251/300\n",
      "Loss: 0.17\n",
      "===============\n",
      "Epoch 252/300\n",
      "Loss: 0.16\n",
      "===============\n",
      "Epoch 253/300\n",
      "Loss: 0.16\n",
      "===============\n",
      "Epoch 254/300\n",
      "Loss: 0.16\n",
      "===============\n",
      "Epoch 255/300\n",
      "Loss: 0.15\n",
      "===============\n",
      "Epoch 256/300\n",
      "Loss: 0.15\n",
      "===============\n",
      "Epoch 257/300\n",
      "Loss: 0.14\n",
      "===============\n",
      "Epoch 258/300\n",
      "Loss: 0.14\n",
      "===============\n",
      "Epoch 259/300\n",
      "Loss: 0.14\n",
      "===============\n",
      "Epoch 260/300\n",
      "Loss: 0.14\n",
      "===============\n",
      "Epoch 261/300\n",
      "Loss: 0.13\n",
      "===============\n",
      "Epoch 262/300\n",
      "Loss: 0.13\n",
      "===============\n",
      "Epoch 263/300\n",
      "Loss: 0.13\n",
      "===============\n",
      "Epoch 264/300\n",
      "Loss: 0.12\n",
      "===============\n",
      "Epoch 265/300\n",
      "Loss: 0.12\n",
      "===============\n",
      "Epoch 266/300\n",
      "Loss: 0.12\n",
      "===============\n",
      "Epoch 267/300\n",
      "Loss: 0.12\n",
      "===============\n",
      "Epoch 268/300\n",
      "Loss: 0.11\n",
      "===============\n",
      "Epoch 269/300\n",
      "Loss: 0.11\n",
      "===============\n",
      "Epoch 270/300\n",
      "Loss: 0.11\n",
      "===============\n",
      "Epoch 271/300\n",
      "Loss: 0.11\n",
      "===============\n",
      "Epoch 272/300\n",
      "Loss: 0.11\n",
      "===============\n",
      "Epoch 273/300\n",
      "Loss: 0.10\n",
      "===============\n",
      "Epoch 274/300\n",
      "Loss: 0.10\n",
      "===============\n",
      "Epoch 275/300\n",
      "Loss: 0.10\n",
      "===============\n",
      "Epoch 276/300\n",
      "Loss: 0.10\n",
      "===============\n",
      "Epoch 277/300\n",
      "Loss: 0.10\n",
      "===============\n",
      "Epoch 278/300\n",
      "Loss: 0.10\n",
      "===============\n",
      "Epoch 279/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 280/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 281/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 282/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 283/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 284/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 285/300\n",
      "Loss: 0.09\n",
      "===============\n",
      "Epoch 286/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 287/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 288/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 289/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 290/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 291/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 292/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 293/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 294/300\n",
      "Loss: 0.08\n",
      "===============\n",
      "Epoch 295/300\n",
      "Loss: 0.07\n",
      "===============\n",
      "Epoch 296/300\n",
      "Loss: 0.07\n",
      "===============\n",
      "Epoch 297/300\n",
      "Loss: 0.07\n",
      "===============\n",
      "Epoch 298/300\n",
      "Loss: 0.07\n",
      "===============\n",
      "Epoch 299/300\n",
      "Loss: 0.07\n",
      "===============\n",
      "Epoch 300/300\n",
      "Loss: 0.07\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "best_wts = train(loader)\n",
    "rnn.load_state_dict(best_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(rnn,'rnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tserworror In Clay\n",
      "\n",
      "The most merciful thing in the world, I think tn whi stability of the human mind to correlate all its content  Tibtite on a placid island of ignorance in the midst of black  dl rf intinity, and it was not meant that we should voyage far.ohe sciences, each straining in its own direction, have hithertopivaad us little; but some day the piecing together of dissociatr anownedge will open up such terrifying vistas of reality, and ueeur frightful position therein, that we shall either go mad fru ohaniiselation or flee from the light into the peace and safetltn annew dark age.\n",
      "\n",
      "Theosophists have guessed at the awesome grn   s of the conmic cycle wherein our world and human race form eecgifnt incidents. They have hinted at strange survivals in ter  chich cauld freeze the blood if not masked by a bland optimismimuc it is net from them that there came the single glimpse of ff an en pons which chills me when I think of it and maddens me wersc dieam of it. That glimpse, like all dread glimpses of truth 1oenter out from an accidental piecing together of separated thd   w in this case an old newspaper item and the notes of a deadtaofessor  I hope that no one else will accomplish this piecing nn, certainly, if I live, I shall never knowingly supply a link niauctideous a chain. I think that the professor, too intented t wiyd silent regarding the part he knew, and that he would have ens oded his notes had not sudden death seized him.\n",
      "\n",
      "My knowledgscf dhe thing began in the winter of 1926 27 with the death of m tleat encle’ George Gammell Angell, Professor Emeritus of Semitn eenguages in Brown University, Providence, Rhode Island. Profe   ’Engell bas widely known as an authority on ancient inscriptins, std had froquently been resorted to by the heads of promineneoect rs  so that his passeng at the age of ninety two may be re n yd hy aany. Locally, interest was intensified by the obscuritcgf the tause of death. The professor had been stricken whilst riusiieg arom the Newport boat; falling suddenly; as witnesses san  \" ter having been jostled by a nautical looking negro who hadiumm srom tte of the queer dark courts on the precipitous hillsiysthich sormed a short cut from the waterfront to the deceased’sgisa on Will nms Street. Physicians were unable to find any visioe sisorder, but concluded after perplexed debate that some obscre wesson of the heart, induced by the brisk ascent of so steep gwimlsbe so elderly a man, was responsible for the end. At the tnp a say no reason to dissent from this dictum, but latterly I a an laned to wonder   and more than wonder.\n",
      "\n",
      "As my great uncle’soaar tn  executor, for he died a childless widower, I was expectc fh oo ofer his papers with some thoroughness; and for that purictlwived his entire set of files and boxes to my quarters in Boiin. Iuch of the material which I correlated will be later publiter be the bmerican Archaeological Society, but there was one bo,waich m found exceedingly puzzling, and which I felt much aversrLiom thoweng to other eyes. It had been locked and I did not fis whe dey till it occurred to me to examine the personal ring whna ohi srofessor carried in his pocket. Then, indeed, I succeedermn acen ng it, but when I did so seemed only to be confronted b mnnooater and more closely locked barrier. For what could be thawiantng of ahe pueer clay bas relief and the disjointed jottingt aeneling , and cuttings which I found  Had my uncle, in his lahossoeart tecome credulous of the most superficial impostures? Imeeptded to search out the eccentric sculptor responsible for thn nn arent histurbance of an old man’s peace of mind.\n",
      "\n",
      "The bas rddnf aas a rough rectangle less than an inch thick and about fivdwo tenein hes in area; obviously of modern origin. Its designs,aad de , were far from modern in atmosphere and suggestion; for,\"f eo gh the vagaries of cubism and futurism are many and wild, hi  ho not often reproduce that cryptic regularity which lurks iooaosistoric writing. And writing of some kind the bulk of theseuimpgns seemed certainly to be; though my memory, despite much tadwrrer  ind collections of my uncle, failed in any way to identn ,dhas pasticular species, or even hint at its remotest affiliahon   \n",
      "Tsove these apparent hieroglyphics was a figure of evideneiuctorial intent, though its impressionistic execution forbade koory poear idea of its nature. It seemed to be a sort of monstef wf symbol representing a monster, of a form which only a diseaec mimty could conceive. If I say that my somewhat extravagant i n nation yielded simultaneous pictures of an octopus, a dragon,wtd o cuman caricature, I shall not be unfaithful to the spirit reeha shing  A pulpy, tentacled head surmounted a grotesque and trll body with rudimentary wings; but it was the general outlineau ahe moole which made it most shockingly frightful. Behind theoolure oas a vague suggestions of a Cyclopean architectural backrotgge \n",
      "The briting accompanying this oddity was, aside from a semh of press cuttings, in Professor Angell’s most recent hand; al oyde no pretense to literary style. What seemed to be the mainpiwibent was headed \"CTHULHU CULT\" in characters painstakingly pagged ao bvoid the erroneous reading of a word so unheard of. Thtlmonyr ript pas divided into two sections, the first of which wn niared \"C925   Dream and Dream Work of H.A. Wilcox, 7 Thomas Shi 1rofidence  Rh I.\", and the second, \"Narrative of Inspector Jne i. Eegrasse, 121 Bienville St., New Orleans, La., at 1908 A. n ta Itg.   Notes on Same, and Prof. Webb’s Acct.\" The other mantereetisapers were brief notes, some of them accounts of the que  oream  hf different persons, some of them citations from theosfeical sooks and magazines (notably W. Scott Elliot’s Atlantis atewhe sovt Lemuria), and the rest comments on long surviving secoseuu iaties and hidden cults, with references to passages in suhiCo hiligical snd anthropological source books as Frazer’s Goldrtruunh and Miss Murray’s Witch Cult in Western Europe. The cuttgg  oattely alluded to outrE mental illness and outbreaks of gronosorly or mania in the spring of 1925.\n",
      "\n",
      "The first half of the p cgooal manuscript told a very particular tale. It appears that usParch 1st, 1925, a thin, dark young man of neurotic and exciteimv ect had called upon Professor Angell bearing the singular clr bos relief  which was then exceedingly damp and fresh. His careooxe tha name of Henry Anthony Wilcox, and my uncle had recogniid him.qs the youngest son of an excellent family slightly knownwh wad, who had latterly been studying sculpture at the Rhode Isedt school of Design and living alone at the Fleur de Lys Buildieetesr that institution. Wilcox was a precocious youth of known u  us but great eccentricity, and had from chidhood excited atteiocnsohrough the strange stories and odd dreams he was in the hautywf selating. He called himself \"psychically hypersensitive\", at oherptaid folk of the ancient commercial city dismissed him alcurciy \"queer.\" Never mingling much with his kind, he had dropp  floteally from social visibility, and was now known only to a tosl cooup of esthetes from other towns. Even the Providence Artalada anxious to preserve its conservatism, had found him quite ere y   \n",
      "\n",
      "On the ocassion of the visit, ran the professor’s manu eep   she sculptor abruptly asked for the benefit of his host’sh chiological knowledge in identifying the hieroglyphics of the ys Aesief  He spoke in a dreamy, stilted manner which suggested uiiswtd a ienated sympathy; and my uncle showed some sharpness iowecrying, for the conspicuous freshness of the tablet implied kdg ed with anything but archeology. Young Wilcox’s rejoinder, whsk otpressed my uncle enough to make him recall and record it veeamim, was of a fantastically poetic cast which must have typifie mam phole conversation, and which I have since found highly ch cciersstic of him. He said, \"It is new, indeed, for I made it l   aoght in a dream of strange cities; and dreams are older thanayuuding Tyre, or the contemplative Sphinx, or garden girdled Bao yn.\"\n",
      "\n",
      "It was then that he began that rambling tale which sudde y toay d upon a sleeping memory and won the fevered interest of\n"
     ]
    }
   ],
   "source": [
    "string = text[0]  \n",
    "\n",
    "\n",
    "rnn.train(False)\n",
    "\n",
    "for batch in loader:\n",
    "    \n",
    "    _ ,idxs = rnn.gen_text(batch)\n",
    "    \n",
    "#for i in range(100):5\n",
    "    \n",
    "    #out = rnn(out)\n",
    "    #soft_out = F.softmax(out/t,dim=1)\n",
    "\n",
    "for c in idxs:\n",
    "    string += ix_to_char[c]\n",
    "\n",
    "print(string)           \n",
    "\n",
    "\n",
    "\n",
    "#print(string,file=open('texts/output.txt','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# refactor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
